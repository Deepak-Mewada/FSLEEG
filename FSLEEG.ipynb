{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObfoheEVCyyFQmffF5o6tM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepak-Mewada/FSLEEG/blob/main/FSLEEG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "QvaXV22rvxHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2uihHVqvkdq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Dataset"
      ],
      "metadata": {
        "id": "CkaKq2EyvyxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Model"
      ],
      "metadata": {
        "id": "bm8_9NEzv07c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import utils\n",
        "import config\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "\n",
        "class SCNN_Model:\n",
        "\n",
        "    def __init__(self, retrain = False, output_dir = '../weights/', batch_size = None, epochs = None, verbose = None):\n",
        "        \"\"\"\n",
        "            Initialize the SCNN model\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            retrain : bool\n",
        "                Whether to retrain the model or not\n",
        "            output_dir : str\n",
        "                Output directory that stores model weights\n",
        "            batch_size : int\n",
        "                Size of batch to train the model\n",
        "            epochs : int\n",
        "                Number of epochs to train the model\n",
        "            verbose : int\n",
        "                Verbosity level\n",
        "\n",
        "        \"\"\"\n",
        "        self.retrain = retrain\n",
        "        self.embedding_size = 128\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.weights_path = os.path.join(output_dir, 'scnn.hdf5')\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Build the SCNN model\n",
        "        self.siamese_network, self.embedding_module, self.relation_module = self.build_SCNN_model()\n",
        "\n",
        "        if(retrain == False):\n",
        "            # Load pretrained weights if retrain is set to False\n",
        "            self.pretrained_weights = os.path.join(output_dir, 'SCNN_pretrained.hdf5')\n",
        "            self.siamese_network.load_weights(self.pretrained_weights)\n",
        "\n",
        "        self.SCNN_accs = []\n",
        "        self.SCNN_recalls = []\n",
        "        self.SCNN_precisions = []\n",
        "        self.SCNN_F1s = []\n",
        "\n",
        "    def build_SCNN_model(self, input_shape = (config.PAD_SIZE, 1)):\n",
        "        \"\"\"\n",
        "            Build the Siamese Network architecture\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            input_shape: tuple\n",
        "                shape of the input data\n",
        "            embedding_size: int\n",
        "                embedding size of the embedding module\n",
        "            maxlen: int\n",
        "                maximum length of the input sequences\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            siamese_network: keras.models.Model\n",
        "                Keras model of the Siamese Network\n",
        "            embedding_module: keras.models.Model\n",
        "                Keras model of the embedding module\n",
        "            relation_module: keras.models.Model\n",
        "                Keras model of the relation module\n",
        "        \"\"\"\n",
        "        tf.keras.backend.clear_session()\n",
        "        left_input = tf.keras.layers.Input(input_shape)\n",
        "        right_input = tf.keras.layers.Input(input_shape)\n",
        "        inputs1 = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "        # Convolutional Block 1\n",
        "        conv1 = tf.keras.layers.Conv1D(filters=128, kernel_size=7, activation='relu', kernel_initializer= 'he_uniform')(inputs1)\n",
        "        bn1 = tf.keras.layers.BatchNormalization()(conv1)\n",
        "        drop1 = tf.keras.layers.Dropout(0.4)(bn1)\n",
        "        pool1 = tf.keras.layers.MaxPooling1D(pool_size= 3)(drop1)\n",
        "        # Convolutional Block 2\n",
        "        conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu', kernel_initializer= 'he_uniform')(pool1)\n",
        "        bn2 = tf.keras.layers.BatchNormalization()(conv2)\n",
        "        drop2 = tf.keras.layers.Dropout(0.4)(bn2)\n",
        "        pool2 = tf.keras.layers.MaxPooling1D(pool_size=3)(drop2)\n",
        "        # Convolutional Block 3\n",
        "        conv3 = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu', kernel_initializer='he_uniform')(pool2)\n",
        "        bn3 = tf.keras.layers.BatchNormalization()(conv3)\n",
        "        drop3 = tf.keras.layers.Dropout(0.4)(bn3)\n",
        "        pool3 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop3)\n",
        "        flat3 = tf.keras.layers.Flatten()(pool3)\n",
        "        # Embedding layer\n",
        "        embedding = tf.keras.layers.Dense(self.embedding_size, activation = 'relu')(flat3)\n",
        "        # Defining the embedding module\n",
        "        embedding_module = tf.keras.Model(inputs=inputs1, outputs=embedding, name = \"Embedding_Module\")\n",
        "\n",
        "        input_l = tf.keras.layers.Input(shape = (self.embedding_size))\n",
        "        input_r = tf.keras.layers.Input(shape = (self.embedding_size))\n",
        "\n",
        "        L1_layer = tf.keras.layers.Lambda(lambda tensors:tf.keras.backend.abs(tensors[0] - tensors[1]))\n",
        "        L1_distance = L1_layer([input_l, input_r])\n",
        "        similarity = tf.keras.layers.Dense(1,activation='sigmoid')(L1_distance)\n",
        "\n",
        "        # Defning the relation module\n",
        "        relation_module = tf.keras.models.Model(inputs = [input_l, input_r], outputs = similarity,\n",
        "                                            name = 'Relation_Module')\n",
        "\n",
        "        embedded_l = embedding_module(left_input)\n",
        "        embedded_r = embedding_module(right_input)\n",
        "        similarity_score = relation_module([embedded_l, embedded_r])\n",
        "\n",
        "        # Defnining the entire Siamese Network\n",
        "        siamese_network = tf.keras.Model(inputs=[left_input, right_input], outputs=similarity_score,\n",
        "                                     name = \"Siamese_Network\")\n",
        "\n",
        "\n",
        "        siamese_network.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50,\n",
        "                                                      min_lr=0.0001)\n",
        "\n",
        "        model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath= self.weights_path, monitor='val_loss', save_best_only=True, verbose = 1)\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "\n",
        "        self.callbacks = [model_checkpoint, early_stopping]\n",
        "\n",
        "        return siamese_network, embedding_module, relation_module\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val):\n",
        "        \"\"\"\n",
        "            Fit the model to the training data\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            x_train: numpy.ndarray\n",
        "                training pairs\n",
        "            y_train: numpy.ndarray\n",
        "                training labels\n",
        "            x_val: numpy.ndarray\n",
        "                validation pairs\n",
        "            y_val: numpy.ndarray\n",
        "                validation labels\n",
        "            batch_size: int\n",
        "                batch size for training\n",
        "            epochs: int\n",
        "                number of epochs\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            hist: History\n",
        "                History of training\n",
        "        \"\"\"\n",
        "\n",
        "        hist = self.siamese_network.fit([ x_train[:,:,0], x_train[:,:,1] ],\n",
        "                                        y_train,\n",
        "                                        batch_size= self.batch_size,\n",
        "                                        epochs= self.epochs,\n",
        "                                        validation_data=([ x_val[:,:,0], x_val[:,:,1] ], y_val),\n",
        "                                        verbose= self.verbose,\n",
        "                                        callbacks=self.callbacks\n",
        "                                        )\n",
        "        return hist\n",
        "\n",
        "    def predict(self, x_train, y_train, x_val, y_val):\n",
        "        \"\"\"\n",
        "            Predict the result of the model on the validation data\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            x_train: numpy.ndarray\n",
        "                training pairs\n",
        "\n",
        "            y_train: numpy.ndarray\n",
        "                training labels\n",
        "\n",
        "            x_val: numpy.ndarray\n",
        "                validation pairs\n",
        "\n",
        "            y_val: numpy.ndarray\n",
        "                validation labels\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            y_pred_train: numpy.ndarray\n",
        "                predictions on the training data\n",
        "\n",
        "            y_pred_val: numpy.ndarray\n",
        "                predictions on the validation data\n",
        "\n",
        "        \"\"\"\n",
        "        if(self.retrain):\n",
        "            self.siamese_network.load_weights(self.weights_path)\n",
        "        else:\n",
        "            self.siamese_network.load_weights(self.pretrained_weights)\n",
        "\n",
        "        y_pred_train = self.siamese_network.predict([ x_train[:,:,0], x_train[:,:,1] ], verbose=1)\n",
        "        y_pred_test = self.siamese_network.predict([ x_val[:,:,0], x_val[:,:,1] ], verbose=1)\n",
        "\n",
        "        return y_pred_train, y_pred_test\n",
        "\n",
        "    def sample_k_shot_task_SCNN(self, X, y, k, q, seed_state_support, seed_state_query):\n",
        "        \"\"\"\n",
        "            Sample a FSL k-shot task for the SCNN model\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            X : pandas.DataFrame\n",
        "                Input data.\n",
        "            y : pandas.Series\n",
        "                Input labels.\n",
        "            k : int\n",
        "                Number of samples per class.\n",
        "            q : int\n",
        "                Number of queries per class.\n",
        "            seed_state_support : int\n",
        "                Seed of support set.\n",
        "            seed_state_query : int\n",
        "                Seed of query set.\n",
        "\n",
        "        \"\"\"\n",
        "        # Initialize the mean vectors, query set and query labels\n",
        "        mean_vecs = np.zeros((y.nunique(), self.embedding_size))\n",
        "        queries = np.empty((0, self.embedding_size))\n",
        "        query_labels = np.empty(0)\n",
        "        i = 0\n",
        "        # Iterate over the unique labels\n",
        "        for label in np.sort(y.unique()):\n",
        "            # Sample k instances of the current label for the support set, and post-pad with zeros\n",
        "            samples = X[y == label].sample(n = k, replace = False, random_state = seed_state_support)\n",
        "            samples_pad = tf.keras.preprocessing.sequence.pad_sequences(samples.values , maxlen = config.PAD_SIZE,\n",
        "                                                                        dtype = 'float64', padding = 'post')\n",
        "\n",
        "            # Obtain embeddings of the padded samples of the support set - f(X)\n",
        "            samples_embedding = self.embedding_module.predict(samples_pad)\n",
        "            # Average the embeddings of the padded samples to get the mean vector of the class mu\n",
        "            class_feature = np.mean(samples_embedding, axis = 0)\n",
        "            # Append the mean vector of the current class to the mean vectors matrix\n",
        "            mean_vecs[i] = class_feature\n",
        "\n",
        "            # Sample q instances of the current label for the query set, and post-pad with zeros\n",
        "            query_samples = X[y == label].sample(n = q, replace = False, random_state = seed_state_query)\n",
        "            query_pad = tf.keras.preprocessing.sequence.pad_sequences(query_samples.values , maxlen = config.PAD_SIZE,\n",
        "                                                                dtype = 'float64', padding = 'post')\n",
        "\n",
        "            # Obtain embeddings of the padded samples of the query set - Q\n",
        "            queries_embedding = self.embedding_module.predict(query_pad)\n",
        "            # Append the embeddings of the query set to queries\n",
        "            queries = np.concatenate((queries, queries_embedding), axis = 0)\n",
        "\n",
        "            # Append the labels of the current class to the query labels\n",
        "            query_labels = np.concatenate((query_labels, np.array([label]*q)))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        return mean_vecs, queries, query_labels\n",
        "\n",
        "    def get_metrics_of_task_SCNN(self, mean_vecs, queries, query_labels):\n",
        "        \"\"\"\n",
        "            Get evaluation metrics of one task of SCNN from support set and query set\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            mean_vecs: numpy.ndarray\n",
        "                Mean vectors of each class of the support set\n",
        "            queries: numpy.ndarray\n",
        "                The query set\n",
        "            query_labels: numpy.ndarray\n",
        "                Labels of the queries\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            acc : float\n",
        "                Accuracy of task\n",
        "            prec : float\n",
        "                Macro-averaged precision of task\n",
        "            rec : float\n",
        "                Macro-averaged recall of task\n",
        "            f1 : float\n",
        "                Macro-averaged F1-score of task\n",
        "        \"\"\"\n",
        "\n",
        "        preds = []\n",
        "        for query in queries:\n",
        "            mult_query = np.resize(query, (5,self.embedding_size))\n",
        "\n",
        "            # Get similarity score g( mu, Q ) for each mean vector mu and and the query embedding Q\n",
        "            similarity_scores = self.relation_module.predict([mean_vecs, mult_query])\n",
        "            # Final prediction of the query is the index of the class with the highest similarity score\n",
        "            preds.append(np.argmax(similarity_scores))\n",
        "\n",
        "        acc = accuracy_score(query_labels, preds)\n",
        "        recall = recall_score(query_labels, preds, average = 'macro')\n",
        "        precision = precision_score(query_labels, preds, average = 'macro')\n",
        "        f1 = f1_score(query_labels, preds, average = 'macro')\n",
        "\n",
        "        return acc, recall, precision, f1\n",
        "\n",
        "    def get_metrics_k_shot_SCNN(self, X, y, k, q, n_tasks):\n",
        "        \"\"\"\n",
        "            Get evaluation metrics of k-shot tasks of the SCNN from a dataset\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            X : pandas.DataFrame\n",
        "                Input data.\n",
        "            y : pandas.Series\n",
        "                Input labels.\n",
        "            k : int\n",
        "                Number of samples per class of support set.\n",
        "            q : int\n",
        "                Number of samples per class of query set.\n",
        "            n_tasks : int\n",
        "                Number of tasks.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            accuracies : numpy.ndarray\n",
        "                List of accuracies of each k-shot task of size (n_tasks, )\n",
        "            recalls : numpy.ndarray\n",
        "                List of macro-averaged recalls of each k-shot task of size (n_tasks, )\n",
        "            precisions : numpy.ndarray\n",
        "                List of macro-averaged precisions of each k-shot task of size (n_tasks, )\n",
        "            f1s : numpy.ndarray\n",
        "                List of macro-averaged F1-scores of each k-shot task of size (n_tasks, )\n",
        "\n",
        "        \"\"\"\n",
        "        accuracies = []\n",
        "        recalls = []\n",
        "        precisions = []\n",
        "        f1s = []\n",
        "        for i in tqdm(range(n_tasks)):\n",
        "            # Sample k-shot task\n",
        "            mean_vecs, queries, query_labels = self.sample_k_shot_task_SCNN(X, y, k, q, seed_state_support= i, seed_state_query=  n_tasks + i)\n",
        "\n",
        "            # Get the evaluation metrics of the k-shot task\n",
        "            acc, recall, precision, f1 = self.get_metrics_of_task_SCNN(mean_vecs, queries, query_labels)\n",
        "\n",
        "            accuracies.append(acc)\n",
        "            recalls.append(recall)\n",
        "            precisions.append(precision)\n",
        "            f1s.append(f1)\n",
        "        return accuracies, recalls, precisions, f1s\n",
        "\n",
        "\n",
        "    def print_metrics_SCNN(self, X_test, y_test, k_min, k_max, num_tasks, q):\n",
        "        \"\"\"\n",
        "            Print evaluation metrics of k-shot tasks of SCNN from a dataset\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            X_test : pandas.DataFrame\n",
        "                Input data.\n",
        "            y_test : pandas.Series\n",
        "                Input labels.\n",
        "            k_min : int\n",
        "                Minimum number of samples per class of support set.\n",
        "            k_max : int\n",
        "                Maximum number of samples per class of support set.\n",
        "            num_tasks : int\n",
        "                Number of SCNN tasks to run.\n",
        "            q : int\n",
        "                Number of samples per class of query set.\n",
        "\n",
        "        \"\"\"\n",
        "        for i in range(k_min, k_max+1):\n",
        "            accuracies, recalls, precisions, f1s= self.get_metrics_k_shot_SCNN(X_test, y_test, k = i, q = q, n_tasks= num_tasks)\n",
        "            print('For %d shot, Accuracy=%.4f, Precision=%.4f, Recall=%.4f, F1=%.4f '%(i, np.mean(accuracies), np.mean(precisions), np.mean(recalls), np.mean(f1s)))\n",
        "\n",
        "            self.SCNN_accs.append(np.mean(accuracies))\n",
        "            self.SCNN_recalls.append(np.mean(recalls))\n",
        "            self.SCNN_precisions.append(np.mean(precisions))\n",
        "            self.SCNN_F1s.append(np.mean(f1s))"
      ],
      "metadata": {
        "id": "7r_sbTHERcaZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}